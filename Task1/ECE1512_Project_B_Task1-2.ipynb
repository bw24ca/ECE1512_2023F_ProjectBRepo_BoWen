{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbRmcIuVhLc2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZueQatn9f3l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbH6g7TXfwfH"
      },
      "source": [
        "2.a MHIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp4WtV5lWNK-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the path for the MHIST dataset\n",
        "mh_dataset_path = '/content/drive/MyDrive/ECE1512/ProjectB/Project_B_Supp/mhist_dataset'\n",
        "\n",
        "# Reading annotation data from the CSV file\n",
        "annotations_file = \"/content/drive/MyDrive/ECE1512/ProjectB/Project_B_Supp/mhist_dataset/annotations.csv\"\n",
        "annotations_data = pd.read_csv(annotations_file, delimiter=',')\n",
        "\n",
        "# Separate data into training and testing based on 'Partition' column\n",
        "training_data = annotations_data[annotations_data['Partition'] == 'train']\n",
        "testing_data = annotations_data[annotations_data['Partition'] == 'test']\n",
        "\n",
        "# Directory path for the image files\n",
        "image_folder = \"/content/drive/MyDrive/ECE1512/ProjectB/Project_B_Supp/mhist_dataset/images\"\n",
        "\n",
        "# Function for image loading and processing\n",
        "def process_image(file_path):\n",
        "    image = Image.open(file_path)\n",
        "    image = image.resize((64, 64))  # Resize images to 64x64 pixels\n",
        "    image_np = np.array(image) / 255.0  # Normalize the pixel values\n",
        "    return image_np\n",
        "\n",
        "# Gather image file paths and their corresponding labels\n",
        "image_files = [os.path.join(image_folder, filename) for filename in annotations_data['Image Name']]\n",
        "labels = annotations_data['Partition'].values\n",
        "\n",
        "# Encoding labels\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "# Splitting dataset into training and testing\n",
        "train_files, test_files, y_train, y_test = train_test_split(\n",
        "    image_files, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Processing images\n",
        "train_images = np.array([process_image(path) for path in tqdm(train_files)])\n",
        "test_images = np.array([process_image(path) for path in tqdm(test_files)])\n",
        "\n",
        "# Model definition\n",
        "class_count = len(np.unique(encoded_labels))  # Number of unique classes\n",
        "mh_model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(class_count, activation='softmax')\n",
        "])\n",
        "\n",
        "# Model compilation\n",
        "mh_model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "# Model training\n",
        "train_history = mh_model.fit(train_images, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Testing the model\n",
        "evaluation_loss, evaluation_accuracy = mh_model.evaluate(test_images, y_test)\n",
        "print(f'Test Accuracy: {evaluation_accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr5BsrVwlp8r"
      },
      "source": [
        "2.a MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyXd1TMaWNA_"
      },
      "outputs": [],
      "source": [
        "# Defining a Feedforward Neural Network\n",
        "''' MLP '''\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc_1 = nn.Linear(28*28*1 if channel==1 else 32*32*3, 128)\n",
        "        self.fc_2 = nn.Linear(128, 128)\n",
        "        self.fc_3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.view(x.size(0), -1)\n",
        "        out = F.relu(self.fc_1(out))\n",
        "        out = F.relu(self.fc_2(out))\n",
        "        out = self.fc_3(out)\n",
        "        return out\n",
        "\n",
        "# Preparing the MNIST dataset\n",
        "dataset_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "mnist_training = datasets.MNIST(root='./data', train=True, download=True, transform=dataset_transform)\n",
        "mnist_testing = datasets.MNIST(root='./data', train=False, download=True, transform=dataset_transform)\n",
        "\n",
        "training_loader = DataLoader(mnist_training, batch_size=64, shuffle=True)\n",
        "testing_loader = DataLoader(mnist_testing, batch_size=64, shuffle=False)\n",
        "\n",
        "# Setting up the neural network\n",
        "ff_network = MLP(input_channels=1, class_count=10)\n",
        "\n",
        "# Loss function, optimizer, and learning rate scheduler\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "sgd_optimizer = optim.SGD(ff_network.parameters(), lr=0.01, momentum=0.9)\n",
        "lr_scheduler = CosineAnnealingLR(sgd_optimizer, T_max=20, eta_min=0.001)\n",
        "\n",
        "# Training procedure\n",
        "def model_training(network, loader, loss_fn, optimizer):\n",
        "    network.train()\n",
        "    for _, (inputs, targets) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        predictions = network(inputs)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Testing procedure\n",
        "def model_testing(network, loader):\n",
        "    network.eval()\n",
        "    total_correct = 0\n",
        "    cumulative_flops = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            predictions = network(inputs)\n",
        "            _, preds = predictions.max(1)\n",
        "            total_correct += preds.eq(targets).sum().item()\n",
        "\n",
        "            # FLOPs Calculation\n",
        "            flops = 2 * 128 * network.flatten_size\n",
        "            cumulative_flops += flops\n",
        "\n",
        "    test_accuracy = total_correct / len(loader.dataset)\n",
        "    flops_per_second = cumulative_flops / len(loader.dataset)\n",
        "\n",
        "    return test_accuracy, flops_per_second\n",
        "\n",
        "# Running the training cycles\n",
        "for epoch in range(20):\n",
        "    model_training(ff_network, training_loader, loss_function, sgd_optimizer)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Evaluating the network\n",
        "accuracy, flops = model_testing(ff_network, testing_loader)\n",
        "print(\"Accuracy of Model: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"FLOPs per Second: {:.2f}\".format(flops))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKP0Oy4FWM7f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e0j8eVeyAtU"
      },
      "source": [
        "2.b MHIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgCV0Pf8t8sh"
      },
      "outputs": [],
      "source": [
        "# Setting up labels from CSV data\n",
        "vote_label = 'Majority Vote Label'\n",
        "\n",
        "# Splitting data based on partitioning\n",
        "train_data = annotations_data[annotations_data['Partition'] == 'train']\n",
        "test_data = annotations_data[annotations_data['Partition'] == 'test']\n",
        "\n",
        "# Encoding labels for the training dataset\n",
        "encoder = LabelEncoder()\n",
        "encoded_train_labels = encoder.fit_transform(train_data[vote_label].values)\n",
        "\n",
        "# Transforming test data labels consistently\n",
        "encoded_test_labels = encoder.transform(test_data[vote_label].values)\n",
        "\n",
        "# Preparing paths for image loading\n",
        "path_images = [os.path.join(image_folder, image_name) for image_name in train_data['Image Name']]\n",
        "\n",
        "# Filtering out unloadable images\n",
        "valid_images = [process_image(path) for path in path_images]\n",
        "valid_indexes = [index for index, img in enumerate(valid_images) if img is not None]\n",
        "\n",
        "# Cleaning data and labels\n",
        "valid_images = [img for img in valid_images if img is not None]\n",
        "encoded_train_labels = encoded_train_labels[valid_indexes]\n",
        "\n",
        "# Data partitioning into training and validation\n",
        "train_imgs, val_imgs, y_train, y_val = train_test_split(\n",
        "    valid_images, encoded_train_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Defining classes and hyperparameters for the Gradient Matching algorithm\n",
        "class_count = len(np.unique(y_train))\n",
        "init_weights = 200\n",
        "iteration_count = 10\n",
        "lr_condensed = 0.1\n",
        "opt_steps_condensed = 1\n",
        "lr_model = 0.01\n",
        "opt_steps_model = 50\n",
        "class_batch = 128\n",
        "\n",
        "# Building a convolutional neural network\n",
        "cnn = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(class_count, activation='softmax')\n",
        "])\n",
        "\n",
        "# Model compilation\n",
        "cnn.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Initialization for synthetic images\n",
        "synth_images = np.random.normal(0, 1, (init_weights, 64, 64, 3))\n",
        "synth_images_tensor = tf.constant(synth_images, dtype=tf.float32)\n",
        "\n",
        "# Implementing the Gradient Matching algorithm\n",
        "for cycle in tqdm(range(iteration_count)):\n",
        "    # Updating the condensed samples\n",
        "    for _ in range(opt_steps_condensed):\n",
        "        with tf.GradientTape() as gtape:\n",
        "            gtape.watch(synth_images_tensor)\n",
        "            synthetic_loss = tf.reduce_sum(cnn(synth_images_tensor))\n",
        "        synthetic_grads = gtape.gradient(synthetic_loss, synth_images_tensor)\n",
        "        synth_images_tensor -= lr_condensed * synthetic_grads.numpy()\n",
        "\n",
        "    synth_images = synth_images_tensor.numpy()\n",
        "\n",
        "    # Model updating\n",
        "    for _ in range(opt_steps_model):\n",
        "        sample_indices = np.random.choice(len(train_imgs), class_batch, replace=False)\n",
        "        batch_imgs = np.array(train_imgs)[sample_indices]\n",
        "        batch_y = y_train[sample_indices]\n",
        "\n",
        "        with tf.GradientTape() as mtape:\n",
        "            model_predictions = cnn(batch_imgs)\n",
        "            model_loss = tf.keras.losses.sparse_categorical_crossentropy(batch_y, model_predictions)\n",
        "\n",
        "        model_grads = mtape.gradient(model_loss, cnn.trainable_variables)\n",
        "        sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "        sgd_optimizer.apply_gradients(zip(model_grads, cnn.trainable_variables))\n",
        "\n",
        "# Cloning the model for evaluation\n",
        "eval_cnn = models.clone_model(cnn)\n",
        "eval_cnn.set_weights(cnn.get_weights())\n",
        "eval_cnn.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare the test dataset for evaluation\n",
        "test_image_paths = [os.path.join(image_folder, name) for name in test_data['Image Name']]\n",
        "test_images = np.array([process_image(path) for path in test_image_paths])\n",
        "test_labels = np.array(encoded_test_labels)\n",
        "\n",
        "# Evaluate and print the accuracy\n",
        "test_accuracy = eval_cnn.evaluate(test_images, test_labels, verbose=2)[1]\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9sniQHY8qWn"
      },
      "outputs": [],
      "source": [
        "# Prepare the test dataset for evaluation\n",
        "test_image_paths = [os.path.join(image_folder, name) for name in test_data['Image Name']]\n",
        "test_images = np.array([process_image(path) for path in test_image_paths])\n",
        "test_labels = np.array(encoded_test_labels)\n",
        "\n",
        "# Evaluate and print the accuracy\n",
        "test_accuracy = eval_cnn.evaluate(test_images, test_labels, verbose=2)[1]\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0APan5XiCbpn"
      },
      "source": [
        "2.b MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-MF3bN1t8kH"
      },
      "outputs": [],
      "source": [
        "def gradient_matching_algorithm(model, dataset, lr_condensed=0.1, num_iterations=10, num_opt_steps=1):\n",
        "    # Define optimizer for condensed samples\n",
        "    optimizer_condensed = optim.SGD(model.parameters(), lr=lr_condensed)\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        # Randomly initialize weights\n",
        "        model.apply(initialize_weights)\n",
        "\n",
        "        # Train on the synthetic dataset\n",
        "        for step in range(num_opt_steps):\n",
        "            # Sample mini-batch from the synthetic dataset\n",
        "            inputs, targets = sample_minibatch(dataset, minibatch_size=256)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer_condensed.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_condensed.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom function for initializing model weights\n",
        "def initialize_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.normal_(layer.weight)\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "# Function to create a condensed dataset\n",
        "def create_condensed_dataset(original_dataset, images_per_class):\n",
        "    selected_indices = []\n",
        "    for class_id in original_dataset.targets.unique():\n",
        "        class_indices = (original_dataset.targets == class_id).nonzero(as_tuple=True)[0][:images_per_class]\n",
        "        selected_indices.extend(class_indices.tolist())\n",
        "    return Subset(original_dataset, selected_indices)\n",
        "\n",
        "# Custom MLP model with random weight initialization\n",
        "class CustomMLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(CustomMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Function to train the model\n",
        "def train_custom_model(custom_model, data_loader, learning_rate=0.01, epochs=10):\n",
        "    optimizer = optim.SGD(custom_model.parameters(), lr=learning_rate)\n",
        "    custom_model.apply(initialize_weights)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = custom_model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Data transformation\n",
        "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, transform=data_transform, download=True)\n",
        "\n",
        "# Condensing the dataset\n",
        "condensed_mnist_train = create_condensed_dataset(mnist_train, 10)\n",
        "\n",
        "# Sampling minibatch\n",
        "def sample_minibatch(dataset, batch_size):\n",
        "    sampler = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return next(iter(sampler))\n",
        "\n",
        "inputs, targets = sample_minibatch(condensed_mnist_train, 256)\n",
        "\n",
        "# Initialize and train the model using gradient matching\n",
        "custom_model = CustomMLP(784, 10)\n",
        "train_custom_model(custom_model, DataLoader(condensed_mnist_train, batch_size=256, shuffle=True), learning_rate=0.01, epochs=20)\n",
        "\n",
        "\n",
        "# Testing model accuracy\n",
        "def test_model_accuracy(test_model, test_data_loader):\n",
        "    correct, total = 0, 0\n",
        "    test_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_data_loader:\n",
        "            output = test_model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, transform=data_transform, download=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
        "accuracy = test_model_accuracy(custom_model, test_loader)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7VkDM4tMN1J"
      },
      "source": [
        "2.c MHIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCNSdxOmX_19"
      },
      "outputs": [],
      "source": [
        "K = 200  # Number of random weight initializations\n",
        "T = 10   # Number of iterations\n",
        "ηS = 0.1  # Learning rate for the condensed samples\n",
        "ζS = 1    # Number of optimization steps for the condensed samples\n",
        "ηθ = 0.01  # Learning rate for the model\n",
        "ζθ = 50   # Number of optimization steps for the model\n",
        "batch_size = 128\n",
        "num_classes = 50\n",
        "\n",
        "# Gather image file paths and their corresponding labels\n",
        "image_files = [os.path.join(image_folder, filename) for filename in annotations_data['Image Name']]\n",
        "labels = annotations_data['Partition'].values\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n",
        "    image_files, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "cnn.compile(optimizer=optimizers.SGD(learning_rate=ηθ), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Initialize condensed images randomly from real training images\n",
        "condensed_images = np.array([process_image(np.random.choice(train_image_paths)) for _ in range(K)])\n",
        "condensed_images_tensor = tf.constant(condensed_images, dtype=tf.float32)  # Convert to TensorFlow Tensor\n",
        "\n",
        "\n",
        "# Gradient Matching algorithm\n",
        "for iteration in tqdm(range(T)):\n",
        "    # Update condensed samples\n",
        "    for _ in range(ζS):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(condensed_images_tensor)\n",
        "            loss_S = tf.reduce_sum(cnn(condensed_images_tensor))\n",
        "        grads_S = tape.gradient(loss_S, condensed_images_tensor)\n",
        "        condensed_images_tensor -= ηS * grads_S.numpy()\n",
        "\n",
        "    # Convert back to NumPy array\n",
        "    condensed_images = condensed_images_tensor.numpy()\n",
        "\n",
        "    # Update model\n",
        "    for _ in range(ζθ):\n",
        "        indices = np.random.choice(len(train_image_paths), batch_size, replace=False)\n",
        "        batch_images = np.array([process_image(train_image_paths[i]) for i in indices])\n",
        "        batch_labels = encoded_labels[indices]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = cnn(batch_images)\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(batch_labels, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, cnn.trainable_variables)\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=ηθ)\n",
        "        optimizer.apply_gradients(zip(gradients, cnn.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrnTwdD0eeXv"
      },
      "outputs": [],
      "source": [
        "# Settings for the plot\n",
        "num_classes = 50  # Total number of classes\n",
        "images_per_class = 1  # Number of images to display per class\n",
        "plot_columns = 10  # Number of columns in the plot\n",
        "\n",
        "# Determine the number of rows needed in the plot\n",
        "plot_rows = (num_classes * images_per_class) // plot_columns\n",
        "if (num_classes * images_per_class) % plot_columns != 0:\n",
        "    plot_rows += 1\n",
        "\n",
        "fig, axes = plt.subplots(plot_rows, plot_columns, figsize=(15, 15))\n",
        "\n",
        "for i in range(plot_rows):\n",
        "    for j in range(plot_columns):\n",
        "        index = i * plot_columns + j\n",
        "        if index < num_classes * images_per_class:\n",
        "            class_index = index // images_per_class\n",
        "            img_index = index % images_per_class\n",
        "            image_idx = class_index * (K // num_classes) + img_index\n",
        "            if image_idx < len(condensed_images):\n",
        "                axes[i, j].imshow(condensed_images[image_idx])\n",
        "                axes[i, j].axis('off')\n",
        "            else:\n",
        "                axes[i, j].axis('off')  # Hide empty subplots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6s4JOUqjWuy"
      },
      "outputs": [],
      "source": [
        "# Initialize condensed images with random noise instead of real images\n",
        "condensed_images = np.random.rand(K, 64, 64, 3)  # Random noise initialization\n",
        "condensed_images_tensor = tf.constant(condensed_images, dtype=tf.float32)\n",
        "\n",
        "# Gradient Matching algorithm\n",
        "for iteration in tqdm(range(T)):\n",
        "    # Update condensed samples\n",
        "    for _ in range(ζS):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(condensed_images_tensor)\n",
        "            loss_S = tf.reduce_sum(model(condensed_images_tensor))\n",
        "        grads_S = tape.gradient(loss_S, condensed_images_tensor)\n",
        "        condensed_images_tensor -= ηS * grads_S.numpy()\n",
        "\n",
        "    condensed_images = condensed_images_tensor.numpy()\n",
        "\n",
        "    # Update model with a batch of real images\n",
        "    for _ in range(ζθ):\n",
        "        indices = np.random.choice(len(train_image_paths), batch_size, replace=False)\n",
        "        batch_images = np.array([process_image(train_image_paths[i]) for i in indices])\n",
        "        batch_labels = train_labels[indices]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(batch_images)\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(batch_labels, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=ηθ)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce7ijoP5jnJ5"
      },
      "outputs": [],
      "source": [
        "# Settings for the plot\n",
        "num_classes = 50  # Total number of classes\n",
        "images_per_class = 1  # Number of images to display per class\n",
        "plot_columns = 10  # Number of columns in the plot\n",
        "\n",
        "# Determine the number of rows needed in the plot\n",
        "plot_rows = (num_classes * images_per_class) // plot_columns\n",
        "if (num_classes * images_per_class) % plot_columns != 0:\n",
        "    plot_rows += 1\n",
        "\n",
        "fig, axes = plt.subplots(plot_rows, plot_columns, figsize=(15, 15))\n",
        "\n",
        "for i in range(plot_rows):\n",
        "    for j in range(plot_columns):\n",
        "        index = i * plot_columns + j\n",
        "        if index < num_classes * images_per_class:\n",
        "            class_index = index // images_per_class\n",
        "            img_index = index % images_per_class\n",
        "            image_idx = class_index * (K // num_classes) + img_index\n",
        "            if image_idx < len(condensed_images):\n",
        "                axes[i, j].imshow(condensed_images[image_idx])\n",
        "                axes[i, j].axis('off')\n",
        "            else:\n",
        "                axes[i, j].axis('off')  # Hide empty subplots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywtvG73bMTcY"
      },
      "source": [
        "2.c MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3csqOs1noxCg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as func\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "# Custom function for initializing network layers\n",
        "def init_network_layers(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.normal_(module.weight)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "# Function to generate a subset dataset\n",
        "def subset_dataset(dataset, imgs_per_class):\n",
        "    subset_indices = []\n",
        "    for label in dataset.targets.unique():\n",
        "        label_indices = (dataset.targets == label).nonzero(as_tuple=True)[0][:imgs_per_class]\n",
        "        subset_indices.extend(label_indices.tolist())\n",
        "    return Subset(dataset, subset_indices)\n",
        "\n",
        "# Defining a custom feed-forward neural network\n",
        "class FeedforwardNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(CustomMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)  # Increased layer size\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Function to train the network\n",
        "def train_network(model, data_loader, lr=0.01, epochs=10):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Data preparation\n",
        "transform_pipeline = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_data = datasets.MNIST(root='./data', train=True, transform=transform_pipeline, download=True)\n",
        "\n",
        "# Creating a condensed version of the dataset\n",
        "condensed_data = subset_dataset(mnist_data, 10)\n",
        "\n",
        "# Function to sample a minibatch from the dataset\n",
        "def sample_batch(dataset, batch_size):\n",
        "    data_sampler = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return next(iter(data_sampler))\n",
        "\n",
        "# Initializing and training the model\n",
        "custom_model = CustomMLP(784, 10)\n",
        "custom_model.apply(initialize_weights)  # Ensure random weight initialization\n",
        "train_custom_model(custom_model, DataLoader(condensed_mnist_train, batch_size=256, shuffle=True), learning_rate=0.01, epochs=20)\n",
        "\n",
        "\n",
        "# Evaluating the model\n",
        "def evaluate_accuracy(model, loader):\n",
        "    correct_count, total_count = 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            output = model(inputs)\n",
        "            _, predicted_labels = torch.max(output, 1)\n",
        "            total_count += labels.size(0)\n",
        "            correct_count += (predicted_labels == labels).sum().item()\n",
        "    return correct_count / total_count\n",
        "\n",
        "\n",
        "mnist_test_set = datasets.MNIST(root='./data', train=False, transform=transform_pipeline, download=True)\n",
        "test_loader = DataLoader(mnist_test_set, batch_size=64, shuffle=False)\n",
        "test_accuracy = evaluate_accuracy(cnn, test_loader)\n",
        "print(f\"Model Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBas42mFL6Sb"
      },
      "outputs": [],
      "source": [
        "# Visualize condensed images\n",
        "def visualize_images(dataset, images_per_class):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i, class_id in enumerate(torch.unique(dataset.dataset.targets)):\n",
        "        indices = torch.where(dataset.dataset.targets == class_id)[0][:images_per_class]\n",
        "        for j, idx in enumerate(indices):\n",
        "            image, _ = dataset.dataset[idx]\n",
        "            plt.subplot(len(torch.unique(dataset.dataset.targets)), images_per_class, i * images_per_class + j + 1)\n",
        "            plt.imshow(image.squeeze(), cmap='gray')\n",
        "            plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_images(condensed_mnist_train, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgRFjTAdWMve"
      },
      "source": [
        "2.d MHIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YLbp4-wbxZw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compile the neural network\n",
        "neural_net = cnn\n",
        "neural_net.compile(optimizer=optimizers.SGD(learning_rate = 0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Process for Gaussian noise initialization\n",
        "noise_iterations = 1  # Adjust as needed\n",
        "for noise_iter in range(noise_iterations):\n",
        "    print(f\"Processing iteration {noise_iter + 1} with Gaussian Noise\")\n",
        "\n",
        "    # Gaussian noise initialization\n",
        "    noise_condensed_images = np.random.normal(0, 1, size=(200, 64, 64, 3))\n",
        "    noise_images_tensor = tf.constant(noise_condensed_images, dtype=tf.float32)\n",
        "\n",
        "    # Apply Gradient Matching\n",
        "    for grad_iter in tqdm(range(10)):\n",
        "        for step in range(1):\n",
        "            with tf.GradientTape() as grad_tape:\n",
        "                grad_tape.watch(noise_images_tensor)\n",
        "                sample_loss = tf.reduce_sum(neural_net(noise_images_tensor))\n",
        "            sample_grads = grad_tape.gradient(sample_loss, noise_images_tensor)\n",
        "            noise_images_tensor -= 0.1 * sample_grads.numpy()\n",
        "\n",
        "        # Convert back to numpy array\n",
        "        noise_condensed_images = noise_images_tensor.numpy()\n",
        "\n",
        "        # Model update\n",
        "        for model_step in range(opt_steps_model):\n",
        "            selected_indices = np.random.choice(len(train_image_paths), 128, replace=False)\n",
        "            images_batch = np.array([process_image(train_image_paths[idx]) for idx in selected_indices])\n",
        "            labels_batch = labels[selected_indices]\n",
        "\n",
        "            with tf.GradientTape() as model_tape:\n",
        "                model_predictions = neural_net(images_batch)\n",
        "                model_loss = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, model_predictions)\n",
        "\n",
        "            model_gradients = model_tape.gradient(model_loss, neural_net.trainable_variables)\n",
        "            model_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "            model_optimizer.apply_gradients(zip(model_gradients, neural_net.trainable_variables))\n",
        "\n",
        "    # Visualization of condensed images\n",
        "num_classes = 50  # Total number of classes\n",
        "images_per_class = 1  # Number of images to display per class\n",
        "plot_columns = 10  # Number of columns in the plot\n",
        "\n",
        "# Determine the number of rows needed in the plot\n",
        "plot_rows = (num_classes * images_per_class) // plot_columns\n",
        "if (num_classes * images_per_class) % plot_columns != 0:\n",
        "    plot_rows += 1\n",
        "\n",
        "fig, axes = plt.subplots(plot_rows, plot_columns, figsize=(15, 15))\n",
        "\n",
        "for i in range(plot_rows):\n",
        "    for j in range(plot_columns):\n",
        "        index = i * plot_columns + j\n",
        "        if index < num_classes * images_per_class:\n",
        "            class_index = index // images_per_class\n",
        "            img_index = index % images_per_class\n",
        "            image_idx = class_index * (K // num_classes) + img_index\n",
        "            if image_idx < len(condensed_images):\n",
        "                axes[i, j].imshow(condensed_images[image_idx])\n",
        "                axes[i, j].axis('off')\n",
        "            else:\n",
        "                axes[i, j].axis('off')  # Hide empty subplots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.d MNIST"
      ],
      "metadata": {
        "id": "Z2fakQ_-JgY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjTHOgyAbxIL"
      },
      "outputs": [],
      "source": [
        "def gradient_matching_algorithm(model, dataset, lr_condensed=0.1, num_iterations=10, num_opt_steps=1):\n",
        "    # Define optimizer for condensed samples\n",
        "    optimizer_condensed = optim.SGD(model.parameters(), lr=lr_condensed)\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        # Randomly initialize weights\n",
        "        model.apply(initialize_weights)\n",
        "\n",
        "        # Train on the synthetic dataset\n",
        "        for step in range(num_opt_steps):\n",
        "            # Sample mini-batch from the synthetic dataset\n",
        "            inputs, targets = sample_minibatch(dataset, minibatch_size=256)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer_condensed.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_condensed.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom function for initializing model weights\n",
        "def initialize_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.normal_(layer.weight)\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "# Function to create a condensed dataset\n",
        "def create_condensed_dataset(original_dataset, images_per_class):\n",
        "    selected_indices = []\n",
        "    for class_id in original_dataset.targets.unique():\n",
        "        class_indices = (original_dataset.targets == class_id).nonzero(as_tuple=True)[0][:images_per_class]\n",
        "        selected_indices.extend(class_indices.tolist())\n",
        "    return Subset(original_dataset, selected_indices)\n",
        "\n",
        "# Custom MLP model with random weight initialization\n",
        "class CustomMLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(CustomMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Function to train the model\n",
        "def train_custom_model(custom_model, data_loader, learning_rate=0.01, epochs=10):\n",
        "    optimizer = optim.SGD(custom_model.parameters(), lr=learning_rate)\n",
        "    custom_model.apply(initialize_weights)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = custom_model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Data transformation\n",
        "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, transform=data_transform, download=True)\n",
        "\n",
        "# Initialize synthetic dataset S with Gaussian noise\n",
        "condensed_images_gaussian = np.random.normal(0, 1, size=(K, 28 * 28))  # Adjust for flattened 28x28 MNIST images\n",
        "\n",
        "# Gradient Matching Algorithm with Gaussian Noise\n",
        "def gradient_matching_gaussian(model, gaussian_images, lr=ηS, num_iters=T, num_opt_steps=ζS):\n",
        "    model.apply(initialize_weights)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for iteration in range(num_iters):\n",
        "        # Update model using synthetic dataset\n",
        "        for step in range(num_opt_steps):\n",
        "            synthetic_data = torch.tensor(gaussian_images, dtype=torch.float32)\n",
        "            synthetic_labels = torch.randint(0, 10, (K,))  # Random labels for synthetic data\n",
        "            outputs = model(synthetic_data)\n",
        "            loss = F.cross_entropy(outputs, synthetic_labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Train the model with Gaussian initialization\n",
        "custom_model = CustomMLP(784, 10)\n",
        "gradient_matching_gaussian(custom_model, condensed_images_gaussian)\n",
        "\n",
        "\n",
        "# Testing model accuracy\n",
        "def test_model_accuracy(test_model, test_data_loader):\n",
        "    correct, total = 0, 0\n",
        "    test_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_data_loader:\n",
        "            output = test_model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, transform=data_transform, download=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
        "accuracy = test_model_accuracy(custom_model, test_loader)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as func\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "# Custom function for initializing network layers\n",
        "def init_network_layers(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.normal_(module.weight)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "# Function to generate a subset dataset\n",
        "def subset_dataset(dataset, imgs_per_class):\n",
        "    subset_indices = []\n",
        "    for label in dataset.targets.unique():\n",
        "        label_indices = (dataset.targets == label).nonzero(as_tuple=True)[0][:imgs_per_class]\n",
        "        subset_indices.extend(label_indices.tolist())\n",
        "    return Subset(dataset, subset_indices)\n",
        "\n",
        "# Defining a custom feed-forward neural network\n",
        "class FeedforwardNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(CustomMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)  # Increased layer size\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Function to train the network\n",
        "def train_network(model, data_loader, lr=0.01, epochs=10):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Data preparation\n",
        "transform_pipeline = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_data = datasets.MNIST(root='./data', train=True, transform=transform_pipeline, download=True)\n",
        "\n",
        "# Creating a condensed version of the dataset\n",
        "condensed_data = subset_dataset(mnist_data, 10)\n",
        "\n",
        "# Function to sample a minibatch from the dataset\n",
        "def sample_batch(dataset, batch_size):\n",
        "    data_sampler = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return next(iter(data_sampler))\n",
        "\n",
        "# Initializing and training the model\n",
        "custom_model = CustomMLP(784, 10)\n",
        "custom_model.apply(initialize_weights)  # Ensure random weight initialization\n",
        "train_custom_model(custom_model, DataLoader(condensed_mnist_train, batch_size=256, shuffle=True), learning_rate=0.01, epochs=20)\n",
        "\n",
        "\n",
        "# Evaluating the model\n",
        "def evaluate_accuracy(model, loader):\n",
        "    correct_count, total_count = 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            output = model(inputs)\n",
        "            _, predicted_labels = torch.max(output, 1)\n",
        "            total_count += labels.size(0)\n",
        "            correct_count += (predicted_labels == labels).sum().item()\n",
        "    return correct_count / total_count\n",
        "\n",
        "\n",
        "mnist_test_set = datasets.MNIST(root='./data', train=False, transform=transform_pipeline, download=True)\n",
        "test_loader = DataLoader(mnist_test_set, batch_size=64, shuffle=False)\n",
        "# Testing model accuracy with Gaussian noise initialized data\n",
        "test_accuracy_gaussian = test_model_accuracy(custom_model, test_loader)\n",
        "print(f'Test Accuracy with Gaussian Initialization: {test_accuracy_gaussian:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "NVS4CjGALWMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize condensed images\n",
        "def visualize_images(dataset, images_per_class):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i, class_id in enumerate(torch.unique(dataset.dataset.targets)):\n",
        "        indices = torch.where(dataset.dataset.targets == class_id)[0][:images_per_class]\n",
        "        for j, idx in enumerate(indices):\n",
        "            image, _ = dataset.dataset[idx]\n",
        "            plt.subplot(len(torch.unique(dataset.dataset.targets)), images_per_class, i * images_per_class + j + 1)\n",
        "            plt.imshow(image.squeeze(), cmap='gray')\n",
        "            plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_images(condensed_mnist_train, 10)"
      ],
      "metadata": {
        "id": "N-LYDDaSYhlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.e MHIST\n",
        "\n",
        "Comparison with Part 2a\n",
        "Test Accuracy: The test accuracy of the model trained on the synthetic dataset (with Gaussian noise initialization) is likely to be lower than the model trained on real images (part 2a). This is because the synthetic dataset does not capture the real data distribution, making it harder for the model to generalize well on actual test images.\n",
        "Training Time: The training time might be similar since the number of epochs and the model architecture are the same. However, the nature of the data could affect how quickly the model converges during training.\n",
        "Explanation of Results\n",
        "Training on Synthetic vs. Real Data: Training on synthetic data is a more challenging task as the model is forced to learn from abstract representations rather than concrete examples. The efficacy of this approach depends on how well the synthetic data captures the underlying patterns of the real data.\n",
        "Generalization Capability: The key aspect to observe is the model's ability to generalize from synthetic to real data. A significant drop in accuracy would indicate limitations in learning from abstract representations.\n",
        "Practical Implications: These results can provide insights into the potential and limitations of using condensed or synthetic datasets for training neural networks, especially in scenarios where real data is scarce or difficult to obtain.\n",
        "In conclusion, this experiment will help in understanding the impact of data quality and representation on the performance of neural networks, particularly in terms of their ability to generalize from synthetic to real-world data."
      ],
      "metadata": {
        "id": "VJV0NdaCt3Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import time\n",
        "\n",
        "# Define the path for the MHIST dataset\n",
        "mh_dataset_path = '/content/drive/MyDrive/ECE1512/ProjectB/Project_B_Supp/mhist_dataset'\n",
        "\n",
        "# Reading annotation data from the CSV file\n",
        "annotations_file = \"/content/drive/MyDrive/ECE1512/ProjectB/Project_B_Supp/mhist_dataset/annotations.csv\"\n",
        "annotations_data = pd.read_csv(annotations_file, delimiter=',')\n",
        "\n",
        "# Separate data into training and testing based on 'Partition' column\n",
        "training_data = annotations_data[annotations_data['Partition'] == 'train']\n",
        "testing_data = annotations_data[annotations_data['Partition'] == 'test']\n",
        "\n",
        "# Directory path for the image files\n",
        "image_folder = \"/content/drive/MyDrive/ECE1512/ProjectB/Project_B_Supp/mhist_dataset/images\"\n",
        "\n",
        "# Function for image loading and processing\n",
        "def process_image(file_path):\n",
        "    image = Image.open(file_path)\n",
        "    image = image.resize((64, 64))  # Resize images to 64x64 pixels\n",
        "    image_np = np.array(image) / 255.0  # Normalize the pixel values\n",
        "    return image_np\n",
        "\n",
        "# Gather image file paths and their corresponding labels\n",
        "image_files = [os.path.join(image_folder, filename) for filename in annotations_data['Image Name']]\n",
        "labels = annotations_data['Partition'].values\n",
        "\n",
        "\n",
        "# Encoding labels\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "# Splitting dataset into training and testing\n",
        "train_files, test_files, y_train, y_test = train_test_split(\n",
        "    image_files, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Processing images\n",
        "train_images = np.array([process_image(path) for path in tqdm(train_files)])\n",
        "test_images = np.array([process_image(path) for path in tqdm(test_files)])\n",
        "# Synthetic Dataset Initialization\n",
        "K = 100  # Number of condensed images\n",
        "synthetic_images = np.random.normal(loc=0, scale=1, size=(K, 64, 64, 3))\n",
        "\n",
        "# Same Model Definition as Part 2a\n",
        "synthetic_model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(class_count, activation='softmax')\n",
        "])\n",
        "synthetic_model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "# Train on Synthetic Dataset\n",
        "start_time = time.time()\n",
        "synthetic_history = synthetic_model.fit(synthetic_images, np.random.randint(0, class_count, K),\n",
        "                                        epochs=20, batch_size=32, validation_split=0.2)\n",
        "synthetic_training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate on Real Testing Data\n",
        "synthetic_test_loss, synthetic_test_accuracy = synthetic_model.evaluate(test_images, y_test)\n"
      ],
      "metadata": {
        "id": "znuMPt9DYoHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.e MNIST\n",
        "\n",
        "Test Accuracy Comparison\n",
        "Training on Synthetic Dataset: The model trained on synthetic data, initialized with Gaussian noise, is expected to have a lower test accuracy when evaluated on real data. This is because the synthetic data does not represent the true distribution and features of the real MNIST dataset.\n",
        "Training on Real Dataset (Part 2a): The model trained on the actual MNIST dataset should have higher accuracy. Real data contains meaningful patterns and features that the model can learn, which are critical for achieving good performance on the test set.\n",
        "Training Time Comparison\n",
        "Training Time: The training time for both scenarios should be relatively similar since the same network architecture, batch size, number of epochs, and optimization settings are used. However, any slight differences might arise from the nature of the data used for training.\n",
        "Explanation of Results\n",
        "Impact of Data Quality on Learning: Training with synthetic data initialized with Gaussian noise represents a situation where the model is exposed to data that lacks meaningful structure or patterns found in real data. As a result, the model might not learn the necessary features to perform well on real data, leading to lower test accuracy.\n",
        "Generalization Capability: The performance on the real test dataset is a measure of how well the model generalizes. The lower performance of the model trained on synthetic data underscores the challenge of generalizing from noisy, non-representative data to real data.\n",
        "Importance of Representative Training Data: The results emphasize the importance of training machine learning models on representative data. Models trained on data that captures the true distribution of the target domain are more likely to perform well.\n",
        "Feasibility of Synthetic Data Training: While training on synthetic data may not yield high accuracy on real data, it's a valuable approach in scenarios where real data is limited or unavailable. The key is to create synthetic data that closely mimics the characteristics of real data.\n",
        "In summary, the experiment demonstrates the critical role of data quality and representativeness in training effective machine learning models. It highlights the challenges of using synthetic data and provides insights into the conditions under which synthetic data might be a viable training option."
      ],
      "metadata": {
        "id": "lTLaPGkEvEGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# MLP class as defined in part 2a\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc_1 = nn.Linear(28 * 28 * channel, 128)\n",
        "        self.fc_2 = nn.Linear(128, 128)\n",
        "        self.fc_3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc_1(x))\n",
        "        x = F.relu(self.fc_2(x))\n",
        "        x = self.fc_3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare synthetic dataset\n",
        "K = 100  # Number of images in the synthetic dataset\n",
        "synthetic_images = torch.randn(K, 1, 28, 28)\n",
        "synthetic_labels = torch.randint(0, 10, (K,))\n",
        "\n",
        "# Initialize the network\n",
        "synthetic_network = MLP(channel=1, num_classes=10)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(synthetic_network.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, images, labels, optimizer, scheduler, epochs=20):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    training_time = time.time() - start_time\n",
        "    return training_time\n",
        "\n",
        "# Train the model on synthetic dataset\n",
        "synthetic_training_time = train_model(synthetic_network, synthetic_images, synthetic_labels, optimizer, scheduler)\n",
        "\n",
        "# Testing on real MNIST data\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the model\n",
        "synthetic_test_accuracy = test_model(synthetic_network, test_loader)\n",
        "\n",
        "# Print results\n",
        "print(f\"Training Time on Synthetic Dataset: {synthetic_training_time:.2f} seconds\")\n",
        "print(f\"Test Accuracy on Real Data: {synthetic_test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "kGKgHFaLt6gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. MHIST"
      ],
      "metadata": {
        "id": "2fiHe8ksAwIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# MHIST Dataset and DataLoader\n",
        "class MHISTCustomDataset(Dataset):\n",
        "    def __init__(self, annotations, directory, transform=None, synthetic_set=None):\n",
        "        self.annotations = annotations\n",
        "        self.directory = directory\n",
        "        self.transform = transform\n",
        "        self.synthetic_set = synthetic_set\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = int(self.annotations.iloc[idx, 2])\n",
        "        if self.synthetic_set is not None:\n",
        "            image = self.synthetic_set[idx]\n",
        "        else:\n",
        "            image_path = os.path.join(self.directory, self.annotations.iloc[idx, 0])\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define transformations\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load and prepare MHIST dataset\n",
        "annotations_path = '/content/drive/MyDrive/ECE1512/ProjectB/Project_B_Supp/mhist_dataset/annotations.csv'\n",
        "images_dir = '/content/drive/MyDrive/ECE1512/ProjectB/Project_B_Supp/mhist_dataset/images'\n",
        "annotations = pd.read_csv(annotations_path)\n",
        "\n",
        "# Split the dataset\n",
        "train_annotations = annotations[annotations['Partition'] == 'train']\n",
        "test_annotations = annotations[annotations['Partition'] == 'test']\n",
        "\n",
        "# Datasets and DataLoaders\n",
        "train_dataset = MHISTCustomDataset(train_annotations, images_dir, transform=image_transforms)\n",
        "test_dataset = MHISTCustomDataset(test_annotations, images_dir, transform=image_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Neural Network Architecture\n",
        "class NeuralNetArch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetArch, self).__init__()\n",
        "        self.layer1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.layer2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 56 * 56, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 128 * 56 * 56)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training and Evaluation Function\n",
        "def train_evaluate_model(model, dataloader, epochs=20, lr=0.01):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for images, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Instantiate, Train, and Evaluate the Model\n",
        "model = NeuralNetArch()\n",
        "accuracy = train_evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "gLvqqji1LD_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. MNIST"
      ],
      "metadata": {
        "id": "9L4HD_oVB0QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST Dataset and DataLoader\n",
        "def load_mnist_data(batch_size=128):\n",
        "    # Define transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Neural Network Architecture for MNIST\n",
        "class MNISTNeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTNeuralNet, self).__init__()\n",
        "        self.layer1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training and Evaluation Function for MNIST\n",
        "def train_evaluate_mnist_model(model, train_loader, test_loader, epochs=10, lr=0.01):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Load data and train the model\n",
        "train_loader, test_loader = load_mnist_data()\n",
        "mnist_model = MNISTNeuralNet()\n",
        "mnist_accuracy = train_evaluate_mnist_model(mnist_model, train_loader, test_loader)\n",
        "mnist_accuracy\n"
      ],
      "metadata": {
        "id": "v3KwIdpjQ-QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4."
      ],
      "metadata": {
        "id": "DHNpea9YF9Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc_1 = nn.Linear(28 * 28 * channel, 128)\n",
        "        self.fc_2 = nn.Linear(128, 128)\n",
        "        self.fc_3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc_1(x))\n",
        "        x = F.relu(self.fc_2(x))\n",
        "        x = self.fc_3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare synthetic dataset\n",
        "K = 100  # Number of images in the synthetic dataset\n",
        "synthetic_images = torch.randn(K, 1, 28, 28)\n",
        "synthetic_labels = torch.randint(0, 10, (K,))\n",
        "\n",
        "# Initialize the network\n",
        "synthetic_network = MLP(channel=1, num_classes=10)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(synthetic_network.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, images, labels, optimizer, scheduler, epochs=20):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    training_time = time.time() - start_time\n",
        "    return training_time\n",
        "\n",
        "# Train the model on synthetic dataset\n",
        "synthetic_training_time = train_model(synthetic_network, synthetic_images, synthetic_labels, optimizer, scheduler)\n",
        "\n",
        "# Testing on real MNIST data\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the model\n",
        "synthetic_test_accuracy = test_model(synthetic_network, test_loader)\n",
        "\n",
        "# Print results\n",
        "print(f\"Training Time on Synthetic Dataset: {synthetic_training_time:.2f} seconds\")\n",
        "print(f\"Test Accuracy on Real Data: {synthetic_test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "ikrpGno7F8zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VohoAJJxWK2e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}