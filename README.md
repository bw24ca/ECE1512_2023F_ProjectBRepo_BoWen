# ECE1512_2023F_ProjectBRepo_BoWen

The project explores dataset distillation methods to reduce computational costs in training deep learning models. It focuses on synthesizing smaller, synthetic datasets that maintain performance comparable to models trained on full datasets. The project is divided into tasks covering Gradient Matching for dataset distillation on MNIST and MHIST datasets, cross-architecture generalization, and application of synthetic datasets in machine learning. The objective is to create a distilled dataset with the most discriminative features of the original, large-scale dataset, and to explore dataset distillation as a data compression technique. The project includes experimental setups, method analysis, and comparison with state-of-the-art methods. 

Reference:https://github.com/GzyAftermath/DATM
